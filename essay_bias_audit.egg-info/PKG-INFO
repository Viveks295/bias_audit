Metadata-Version: 2.4
Name: essay_bias_audit
Version: 0.1.0
Summary: A package to audit bias in essay grading models
Author: 
Author-email: 
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: click
Requires-Dist: nltk
Requires-Dist: deep-translator
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# essay_bias_audit

A Python package to audit bias in essay grading models. It allows users to grade essays with their model, apply
text variations (e.g., spelling errors, code-switching), and analyze how these variations affect model grading.

## Installation

```bash
pip install .
# Needed for translation variations
pip install deepl nltk
```

## Usage

```python
import pandas as pd
from essay_bias_audit.auditor import Auditor

# Load data
df = pd.read_csv('essays.csv')

# Define grading model: function taking (prompt, essay) and returning grade
def my_model(prompt, essay):
    # model code here
    return ...

# Create auditor and grade original essays
auditor = Auditor(model=my_model, data=df)
original_grades = auditor.grade()
print('Accuracy:', auditor.accuracy())

# Audit bias
variations = ['spelling', 'spanglish']
magnitudes = [30, 50]
report = auditor.audit(variations, magnitudes)
print(report.head())
```

## CLI

Make sure to set your DeepL API key as an environment variable:
```bash
export DEEPL_API_KEY=your_deepl_api_key
```
```bash
essay-bias-audit --data essays.csv --model-script model.py --model-func grade \
  --variations spelling --magnitudes 30 \
  --variations spanglish --magnitudes 50 \
  --output audit_results.csv
```
